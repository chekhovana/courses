{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"imdb_classification_basic.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/chekhovana/courses/blob/main/ml_stepik/6_final_project/week4_online_app/model/imdb_classification_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ay0R0a6gS_aR"},"source":["##Install and import libraries"]},{"cell_type":"code","metadata":{"id":"Ap0ouhVV_5-9"},"source":["!pip install gensim==4.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6IHaZ9NCzvb"},"source":["import re\n","import os\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","\n","from xgboost import XGBClassifier\n","\n","# import nltk\n","\n","from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmHsZyBnkDP0"},"source":["#Prepare data"]},{"cell_type":"markdown","metadata":{"id":"xLH7EAV3U9Th"},"source":["##Load dataset"]},{"cell_type":"code","metadata":{"id":"ICSIK1J2SZcI"},"source":["url = \"https://github.com/chekhovana/courses/raw/main/ml_stepik/6_final_project/week4_online_app/model/data/imdb_preprocessed.csv\"\n","df = pd.read_csv(url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bKbLxeSMuOf"},"source":["##Create train dataset for exploration purposes"]},{"cell_type":"markdown","metadata":{"id":"DAUKZP6ZkYvC"},"source":["When evaluating different vectorization and classification algorithms, to speed up the training process, limited dataset of 5000 records will be used"]},{"cell_type":"code","metadata":{"id":"Q9SHRoG8TbMC"},"source":["n_train = 5000\n","df_train = df[:n_train]\n","x_train, y_train = df_train['review'].values, df_train['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGKgT6oWYKhE"},"source":["Check train sample for class balance"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvPZmpaxYQe0","outputId":"ec741c74-b45d-4264-d929-0a20266b72fd"},"source":["print('class ratio', np.sum(y_train) / len(y_train))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["class ratio [0.503]\n"]}]},{"cell_type":"markdown","metadata":{"id":"m-hFmzriYvif"},"source":["The sample is balanced"]},{"cell_type":"markdown","metadata":{"id":"i73bhW1LNYZA"},"source":["#Select model"]},{"cell_type":"markdown","metadata":{"id":"swnK5e8hSa-G"},"source":["##Compare classifiers"]},{"cell_type":"markdown","metadata":{"id":"pUnCNUFqSiKc"},"source":["Compare different classifiers by accuracy. The dataset of limited size of 5000 records is used to speed up training process. Text is vectorized with 'bag of words' algorithm with maximum number of features set to 10000."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXsbMh9M58BF","outputId":"bc7e2daa-aeaa-45f0-becc-80d1017ae970"},"source":["%%time\n","max_features = 10000\n","\n","\n","def compare_models(x, y):\n","    vectorizer = CountVectorizer(max_features=max_features)\n","    x = vectorizer.fit_transform(x)\n","    models = {}\n","    models['LogisticRegression'] = LogisticRegression(max_iter=1000)\n","    models['SVC(kernel=\"rbf\")'] = SVC()\n","    models['SVC(kernel=\"linear)'] = SVC(kernel='linear')\n","    models['RandomForestClassifier'] = RandomForestClassifier()\n","    models['MultinomialNB'] = MultinomialNB()\n","    for mname, model in models.items():\n","        score = cross_val_score(model, x, y, cv=3).mean()\n","        print(mname, round(score, 4))\n","\n","compare_models(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression 0.8394\n","SVC(kernel=\"rbf\") 0.8198\n","SVC(kernel=\"linear) 0.8166\n","RandomForestClassifier 0.8342\n","MultinomialNB 0.8302\n","CPU times: user 1min 9s, sys: 1.25 s, total: 1min 11s\n","Wall time: 1min 9s\n"]}]},{"cell_type":"markdown","metadata":{"id":"M6Nbn1YKQZET"},"source":["###Conclusion"]},{"cell_type":"markdown","metadata":{"id":"qROboMIdQeH5"},"source":["The best classifier is LogisticRegression, it will be used in the rest of this notebook"]},{"cell_type":"code","metadata":{"id":"FAv2XDHklyG4"},"source":["model = LogisticRegression(max_iter=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJ61I-YbVesu"},"source":["##Compare vectorization approaches"]},{"cell_type":"markdown","metadata":{"id":"XcN-gYOLgQjp"},"source":["### CountVectorizer and TfidfVectorizer with different ranges of n-gram extraction"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNEEjMfaXedl","outputId":"2f107a62-71b4-4d09-cd7f-efe0550c9dda"},"source":["def compare_vectorizers(x, y):\n","    vectorizers = {}\n","    for vectorizer in (CountVectorizer, TfidfVectorizer):\n","        for ngram in range(1, 4):\n","            vname = f'{vectorizer.__name__}(ngram_range=(1, {ngram}))'\n","            vectorizers[vname] = vectorizer(max_features=max_features, \n","                                            ngram_range=(1, ngram))\n","\n","    for vname, vectorizer in vectorizers.items():\n","        x_train_vectorized = vectorizer.fit_transform(x)\n","        score = round(cross_val_score(model, x_train_vectorized, y).mean(), 4)\n","        print(vname, score)\n","\n","compare_vectorizers(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CountVectorizer(ngram_range=(1, 1)) 0.8412\n","CountVectorizer(ngram_range=(1, 2)) 0.8434\n","CountVectorizer(ngram_range=(1, 3)) 0.8418\n","TfidfVectorizer(ngram_range=(1, 1)) 0.8618\n","TfidfVectorizer(ngram_range=(1, 2)) 0.8596\n","TfidfVectorizer(ngram_range=(1, 3)) 0.8598\n"]}]},{"cell_type":"markdown","metadata":{"id":"AO14jRb2iVGx"},"source":["### Gensim implementation of n-gram extraction"]},{"cell_type":"markdown","metadata":{"id":"qeyUIKRtlVoN"},"source":["Using gensim, extract bigrams and trigrams and feed obtained text to vectorizers - CountVectorizer and TfidfVectorizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JVKfhBTg8Qi","outputId":"f9ccad16-fd63-40d9-e9a0-9252542ebeae"},"source":["def extract_trigrams(x):\n","    x = [o.split() for o in x]\n","\n","    params = dict(min_count=1, threshold=1, \n","                  connector_words=ENGLISH_CONNECTOR_WORDS)\n","    \n","    bigram_model = Phrases(x, **params)\n","    bigrams = bigram_model[x]\n","\n","    trigram_model = Phrases(bigrams, **params)\n","    trigrams = trigram_model[bigrams]\n","    return [' '.join(t) for t in trigrams]    \n","\n","def evaluate_gensim_trigrams(x_train, y_train):\n","    x_train = extract_trigrams(x_train)\n","    vectorizers = {}\n","    for vectorizer in (CountVectorizer, TfidfVectorizer):\n","        vname = vectorizer.__name__\n","        vectorizers[vname] = vectorizer(max_features=max_features)\n","\n","    model = LogisticRegression(max_iter=1000)\n","    for vname, vectorizer in vectorizers.items():\n","        x_train_vectorized = vectorizer.fit_transform(x_train)\n","        score = round(cross_val_score(model, x_train_vectorized, y_train).mean(), 4)\n","        print(vname, score)\n","\n","evaluate_gensim_trigrams(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CountVectorizer 0.7896\n","TfidfVectorizer 0.8166\n"]}]},{"cell_type":"markdown","metadata":{"id":"PW52hUgl_edo"},"source":["### Word2vec embeddings, gensim implementation"]},{"cell_type":"markdown","metadata":{"id":"ojyi59hZmDL9"},"source":["Create Word2Vec model and train it on our corpus"]},{"cell_type":"code","metadata":{"id":"sO4X6DADxShG"},"source":["import gensim\n","from gensim.models.word2vec import Word2Vec\n","\n","def create_word2vec_model(x):\n","    x = [o.split() for o in x]\n","    dim = 300\n","    model = Word2Vec(x, vector_size=dim, window=5, min_count=1)\n","    model.train(x, total_examples=len(x), epochs=10)\n","    return model\n","\n","word2vec = create_word2vec_model(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTAuOuKwoPOD"},"source":["Naive approach: vector representation of sentence is calculated by averaging over the vector representations of all its words.The better way is to feed obtained 3d-representation of the sentence to the neural net, it's to be implemented in another notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zu8A_x27kD0N","outputId":"4eee48ad-38eb-48c8-fc89-2e96b3cb89ba"},"source":["def vectorize_sentence(sentence):\n","    words = sentence.split()\n","    embeddings = [word2vec.wv[w] for w in words]\n","    return np.mean(embeddings, axis=0)\n","\n","x_vectorized = np.array(list(map(vectorize_sentence, x_train)))\n","score = round(cross_val_score(model, x_vectorized, y_train).mean(), 4)\n","print(score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8178\n"]}]},{"cell_type":"markdown","metadata":{"id":"kNqS68HJmssp"},"source":["### Conclusion"]},{"cell_type":"markdown","metadata":{"id":"vmfj2Mm3m-3c"},"source":["The best vectorization model is TfidfVectorizer(ngram_range=(1, 1))"]},{"cell_type":"markdown","metadata":{"id":"chqZ1RLFnTHC"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"HAHGT0C2nXl2"},"source":["Create pipeline with the best vectorizer and classifier. Evaluate it on the whole dataset using cross-validation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XzIiPXs_nwgK","outputId":"c539ec38-9260-4aa3-ca68-6ce0c5bdbf1d"},"source":["pline = Pipeline([('vectorizer', TfidfVectorizer(max_features=max_features)), \n","                  ('classifier', model)])\n","# pline.fit(x_train, y_train)\n","print(round(cross_val_score(pline, x_train, y_train).mean(), 4))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8871\n","CPU times: user 33.7 s, sys: 6.28 s, total: 40 s\n","Wall time: 32.8 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"VTbk9ZoGbv2p"},"source":["###Save model"]},{"cell_type":"markdown","metadata":{"id":"LchhJdU7aTFs"},"source":["Train pipeline on the whole dataset and save it for future use"]},{"cell_type":"code","metadata":{"id":"BChVFBs_Z6kq"},"source":["import pickle\n","pline.fit(x_train, y_train)\n","with open(\"sentiment-classifier.pickle\", \"wb\") as f:\n","    pickle.dump(pline, f)"],"execution_count":null,"outputs":[]}]}