{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"param_tuning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Q-5UeF4Tus2AygCtDVMC51OHW8LS41Wn","authorship_tag":"ABX9TyMH2lLyEXFW/BH/+SI5TzUl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"],"metadata":{"id":"_psoWSCPq3Dq"}},{"cell_type":"markdown","metadata":{"id":"syML1iPSw_Ee"},"source":["# Настройка параметров\n","В этом задании вам предстоит поэкспериментировать с параметрами вашей модели для сентимент-анализа. Все задания выполняются на том же датасете, что и на прошлой неделе.\n","\n","## Инструкции\n","\n","1. Здесь и далее оценка качества будет выполняться с помощью cross_val_score с cv=5 и остальными параметрами по умолчанию. Оцените среднее качество ( .mean() ) и стандартное отклонение ( .std() ) по fold'ам для: а) pipeline из CountVectorizer() и LogisticRegression(), б) pipeline из TfidfVectorizer() и LogisticRegression(). В соответствующем пункте задания выпишите через пробел среднее в п. а, отклонение в п. а, среднее в п.б и отклонение в п. б\n","\n","2. Попробуйте задавать разные значения параметра min_df у CountVectorizer. Оцените качество вашего классификатора с min_df=10 и с min_df=50.\n","\n","3. Попробуйте использовать разные классификаторы после CountVectorizer. И vectorizer, и классификатор берите с параметрами по умолчанию. Сравните результаты для LogisticRegression, LinearSVC и SGDClassifier. Выпишите в ответе на соответствующий вопрос самое худшее качество из получившихся.\n","\n","4. Подготовьте список стоп-слов с помощью nltk.corpus.stopwords.words('english'), посмотрите на его элементы, и передайте его в соответствующий параметр CountVectorizer. В sklearn также предусмотрен свой список английских стоп-слов - для этого нужно задать соответствующий параметр равным строке 'english'. Оцените качество классификатора в одном и другом случае и выпишете сначала качество в первом варианте, затем во втором в соответствующем вопросе.\n","\n","5. Попробуйте в CountVectorizer добавить к словам биграммы и измерить качество модели. А затем постройте модель на частотах буквенных n-грамм c n от 3 до 5, указав соответствующее значение параметра ngram_range и параметр analyzer='char_wb'. Полученные два числа запишите через пробел в ответе на соответствующий вопрос."]},{"cell_type":"markdown","metadata":{"id":"E3PRWZic55fz"},"source":["Откатываем на нужную версию sklearn (иначе ответы не принимаются)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tza7jwvhTxT","executionInfo":{"status":"ok","timestamp":1634202746442,"user_tz":-180,"elapsed":3279,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"a23ffb63-9bd2-4a38-f30e-604a0f38dc1c"},"source":["!pip install scikit-learn==0.20.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn==0.20.1 in /usr/local/lib/python3.7/dist-packages (0.20.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.1) (1.19.5)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.1) (1.4.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"DmhwTbn36Dux"},"source":["Импортируем библиотеки"]},{"cell_type":"code","metadata":{"id":"SshozRvtA6I_"},"source":["import numpy as np\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import cross_val_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZlsujh4a9VL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634202747183,"user_tz":-180,"elapsed":354,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"7a5134b0-3efd-4160-ac5d-1f725025a23b"},"source":["import nltk\n","from nltk.corpus import movie_reviews\n","from nltk.corpus import stopwords\n","nltk.download('movie_reviews')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"6SwV3_FFBM2y"},"source":["---\n","Загружаем данные"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AZGbFb7I9-A","executionInfo":{"status":"ok","timestamp":1634202748885,"user_tz":-180,"elapsed":1705,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"b3660de1-a414-4366-bc1f-17eccd30a37c"},"source":["negids = movie_reviews.fileids('neg') \n","posids = movie_reviews.fileids('pos')\n","print('negative', len(negids), 'positive', len(posids))\n","neg_reviews = [' '.join(movie_reviews.words(fileids=[f])) for f in negids]\n","pos_reviews = [' '.join(movie_reviews.words(fileids=[f])) for f in posids]\n","corpus = neg_reviews + pos_reviews\n","labels = [0] * len(neg_reviews) + [1] * len(pos_reviews)\n","print('total', len(corpus))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["negative 1000 positive 1000\n","total 2000\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5bnqhSu6t3H"},"source":["---\n","Оценим пайплайн из CountVectorizer и LogisticRegression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJ0uf2KD_7sj","executionInfo":{"status":"ok","timestamp":1634202756281,"user_tz":-180,"elapsed":7403,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"abc1ac60-ca01-4e1d-a02b-04d64a83d9c2"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","vectorizer = CountVectorizer()\n","clf = LogisticRegression(max_iter=1000)\n","pline = Pipeline([(\"vectorizer\", vectorizer), (\"classifier\", clf)])\n","res = cross_val_score(pline, corpus, labels, cv=5)\n","print(res.mean(), res.std())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8415000000000001 0.01677796173556255\n"]}]},{"cell_type":"markdown","metadata":{"id":"j4saVjDx64LO"},"source":["---\n","Оценим пайплайн из TfidfVectorizer и LogisticRegression"]},{"cell_type":"code","metadata":{"id":"a0NVagu2XO8T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634202762245,"user_tz":-180,"elapsed":5984,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"074dcdc5-f381-4094-c48d-ebedbf591a21"},"source":["pline = Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', clf)])\n","res = cross_val_score(pline, corpus, labels, cv=5)\n","print(res.mean(), res.std())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8210000000000001 0.004062019202317978\n"]}]},{"cell_type":"markdown","metadata":{"id":"ju-U_7r_7tMO"},"source":["---\n","Проанализируем влияние параметра min_df (минимально необходимое количество вхождений слова в корпус документов) на точность классификации"]},{"cell_type":"code","metadata":{"id":"2QD7sXhhcUBj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634202774979,"user_tz":-180,"elapsed":12754,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"7ab2096f-5f7f-4916-c990-e81daf664266"},"source":["for min_df in (10, 50):\n","    pline = Pipeline([('vectorizer', CountVectorizer(min_df=min_df)), ('classifier', clf)])\n","    print(cross_val_score(pline, corpus, labels, cv=5).mean())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8390000000000001\n","0.813\n"]}]},{"cell_type":"markdown","metadata":{"id":"87PvYL2CBTd-"},"source":["---\n","Сравним различные алгоритмы классификации по точности"]},{"cell_type":"code","metadata":{"id":"4XYNqUo9dBCV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634202796906,"user_tz":-180,"elapsed":21949,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"a66cb917-d002-4d61-d5d1-db2d75f40abe"},"source":["classifiers = LogisticRegression(), LinearSVC(), SGDClassifier()\n","for clf in classifiers:\n","    # print(clf)\n","    pline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', clf)])\n","    print(clf.__class__.__name__, cross_val_score(pline, corpus, labels, cv=5).mean())    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression 0.8415000000000001\n","LinearSVC 0.8325000000000001\n","SGDClassifier 0.7790000000000001\n"]}]},{"cell_type":"markdown","metadata":{"id":"TILe4k3FC-J-"},"source":["---\n","Векторизация с использованием списка стоп-слов из библиотеки nltk"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfPas9ZUuN4R","executionInfo":{"status":"ok","timestamp":1634202803199,"user_tz":-180,"elapsed":6311,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"87f8e217-69e7-4d01-dc88-ad0921f85732"},"source":["nltk_stopwords = stopwords.words('english')\n","vectorizer = CountVectorizer(stop_words=nltk_stopwords)\n","pline = Pipeline([('vectorizer', vectorizer), \n","                  ('classifier', LogisticRegression(max_iter=1000))])\n","print(cross_val_score(pline, corpus, labels, cv=5).mean())    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8414999999999999\n"]}]},{"cell_type":"markdown","metadata":{"id":"6JKRCYPuDSEp"},"source":["---\n","Векторизация с использованием списка стоп-слов из библиотеки sklearn"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAleJaG9wKWP","executionInfo":{"status":"ok","timestamp":1634202809498,"user_tz":-180,"elapsed":6307,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"e34d1a9f-5cf6-4ae6-d9e5-4c4b29e14f34"},"source":["vectorizer = CountVectorizer(stop_words='english')\n","pline = Pipeline([('vectorizer', vectorizer), \n","                  ('classifier', LogisticRegression(max_iter=1000))])\n","print(cross_val_score(pline, corpus, labels, cv=5).mean())    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8390000000000001\n"]}]},{"cell_type":"markdown","metadata":{"id":"0nA52gdgD4WX"},"source":["---\n","Векторизация с добавлением биграмм"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xjN2J0cySHc","executionInfo":{"status":"ok","timestamp":1634202848184,"user_tz":-180,"elapsed":38709,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"7b608b6b-9c8a-4976-9434-403d30ea7984"},"source":["pline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))), \n","                  ('classifier', LogisticRegression(max_iter=1000))])\n","print(cross_val_score(pline, corpus, labels, cv=5).mean())    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8525\n"]}]},{"cell_type":"markdown","metadata":{"id":"RQXcbQEDEDeY"},"source":["---\n","Векторизация буквенных n-грамм"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8-JbM0Nwyyk","executionInfo":{"status":"ok","timestamp":1634202938973,"user_tz":-180,"elapsed":90810,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"d6445a64-d480-402d-9d89-2e9d61bf6643"},"source":["pline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(3, 5), analyzer='char_wb')), \n","                  ('classifier', LogisticRegression(max_iter=1000))])\n","print(cross_val_score(pline, corpus, labels, cv=5).mean())    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.82\n"]}]}]}