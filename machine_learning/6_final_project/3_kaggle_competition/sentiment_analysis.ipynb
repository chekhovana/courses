{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1jd3CybU6foHkYw7EMOtPyjpCoPwmwL2J","authorship_tag":"ABX9TyMOJWGiyYnoz+vzxnROhT2z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"c8qDF_Kyr6JX"},"source":["В данном ноутбуке использована архитектура BERT - предобученная модель, продемонстрировавшая высокую эффективность в решении ряда NLP-задач. В рамках нашего курса модель не рассматривалась, вот подборка ссылок на ресурсы для ознакомления с темой:\n","\n","* Оригинальные статьи\n","    * [Ilya Sutskever. Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n","    * [Dzmitry Bahdanau. Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n","    * [Ashish Vaswani. Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","    * [Jacob Devlin. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n","* Дополнительные материалы\n","    * [Jay Alammar. Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n","    * [Jay Alammar. The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n","    * [Chaitanya Joshi. Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/)\n","    * [Jay Alammar. The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)\n","<br><br>\n","---\n","В качестве шаблона при разработке ноутбука был использован [демо-пример по работе с BERT с официального сайта tensorflow](https://www.tensorflow.org/text/tutorials/bert_glue).\n","\n","Модель содержит большое число параметров и требовательна к ресурсам, что обуславливает необходимость использования тензорного процессора (TPU) в Google Colab. Переход на TPU доступен через главное меню Colab: Runtime -> Change runtime type"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rE2dA1XO9clA","executionInfo":{"status":"ok","timestamp":1630904235194,"user_tz":-180,"elapsed":11829,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"90e17d8e-d0e9-4c75-afca-9d05b00f9a9c"},"source":["!pip install -q -U tensorflow-text\n","!pip install -q -U tf-models-official\n","!pip install -q -U tfds-nightly"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.4.0.dev202109050107)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.17.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.62.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.2.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.2.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tfds-nightly) (3.5.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n"]}]},{"cell_type":"code","metadata":{"id":"_XgTpm9ZxoN9"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","import tensorflow_addons as tfa\n","\n","from official.nlp import optimization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYj7vkOWq9uu"},"source":["tf.get_logger().setLevel('ERROR')\n","\n","#необходимо загружать модели в распакованном виде, поскольку при использовании TPU нет \n","#возможности загрузить архив и сохранить его на локальный диск в распакованном виде\n","os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n","\n","# модель состоит из двух блоков - препроцессора и энкодера, которые могут быть загружены из \n","# репозитория tfhub по указанным ссылкам\n","preprocessor_name = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'\n","encoder_name = 'https://tfhub.dev/tensorflow/albert_en_xxlarge/3'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTnPQnIx9Btg"},"source":["Подключение к TPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9izk8UZ-jcf0","executionInfo":{"status":"ok","timestamp":1630904255454,"user_tz":-180,"elapsed":17221,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"ea31af54-fb75-40ea-e18d-bcdb7311da81"},"source":["cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","tf.config.experimental_connect_to_cluster(cluster_resolver)\n","tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","strategy = tf.distribute.TPUStrategy(cluster_resolver)\n","print('Using TPU')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using TPU\n"]}]},{"cell_type":"markdown","metadata":{"id":"vOndpWH7rZ5B"},"source":["Загрузка данных"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JN4N5KEjWokh","executionInfo":{"status":"ok","timestamp":1630904255806,"user_tz":-180,"elapsed":372,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"8bb50698-4764-4895-bd1c-0eab8286e844"},"source":["path = 'https://raw.githubusercontent.com/chekhovana/data/main/stepik/module6'\n","path = os.path.join(path, 'products_sentiment_{dataset}.tsv')\n","fname_train, fname_test = (path.format(dataset=ds) for ds in ('train', 'test'))\n","\n","data = pd.read_csv(fname_train, sep='\\t', header=None)\n","train_size = 1600\n","valid_size = data.shape[0] - train_size\n","train_data = data[:train_size]\n","valid_data = data[-valid_size:]\n","print(train_data.shape, valid_data.shape)\n","\n","batch_size = 32"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1600, 2) (400, 2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"vq_A-4nwqtAO"},"source":["Функция для создания препроцессора. Он будет приводить входные данные к тому формату, в котором они должны подаваться на вход энкодера"]},{"cell_type":"code","metadata":{"id":"nUR2dSETbWnu"},"source":["def create_preprocessor(name, seq_length=128):\n","    input_segments = [tf.keras.layers.Input(shape=(), dtype=tf.string, \n","                                            name='sentence')]\n","    preprocessor = hub.load(name)\n","    tokenizer = hub.KerasLayer(preprocessor.tokenize, name='tokenizer')\n","    segments = [tokenizer(s) for s in input_segments]\n","\n","    packer = hub.KerasLayer(preprocessor.bert_pack_inputs,\n","                            arguments=dict(seq_length=seq_length),\n","                            name='packer')\n","    model_inputs = packer(segments)\n","    return tf.keras.Model(input_segments, model_inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVadS5HHq7Dq"},"source":["Функция для создания датасета, преобразованного к требуемому энкодером формату. Для переформатирования данных используется передаваемый в качестве аргумента препроцессор"]},{"cell_type":"code","metadata":{"id":"TwmYdM3FboQp"},"source":["def create_dataset(data, preprocessor, is_train=True):\n","    dataset = tf.data.Dataset.from_tensor_slices(\n","        {'sentence': data[0].values, 'label': data[1].values})\n","    if (is_train):\n","        dataset = dataset.shuffle(data.shape[0])\n","        dataset = dataset.repeat()\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(lambda ex: (preprocessor(ex), ex['label']))\n","    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFaCe7MvP0FI"},"source":["Функция для создания энкодера. В качестве параметра передается ссылка на предобученную модель."]},{"cell_type":"code","metadata":{"id":"CEkY2tbJHA8a"},"source":["def create_encoder(name):\n","\n","  class Classifier(tf.keras.Model):\n","    def __init__(self):\n","        super(Classifier, self).__init__(name=\"prediction\")\n","        self.encoder = hub.KerasLayer(name, trainable=True)\n","        self.dropout = tf.keras.layers.Dropout(0.1)\n","        self.dense = tf.keras.layers.Dense(2)\n","\n","    def call(self, preprocessed_text):\n","      encoder_outputs = self.encoder(preprocessed_text)\n","      pooled_output = encoder_outputs[\"pooled_output\"]\n","      x = self.dropout(pooled_output)\n","      x = self.dense(x)\n","      return x\n","\n","  return Classifier()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2bigOrMQZbW"},"source":["Обучение модели."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUzAIwZOgA4D","executionInfo":{"status":"ok","timestamp":1630904728262,"user_tz":-180,"elapsed":472462,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"e0fce93d-4f3f-41e4-e59d-df9eef09e989"},"source":["%%time\n","\n","batch_size = 32\n","init_lr = 2e-5\n","epochs = 20\n","\n","steps_per_epoch = train_size // batch_size\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = num_train_steps // 10\n","validation_steps = valid_size // batch_size\n","\n","optimizer = optimization.create_optimizer(\n","    init_lr=init_lr,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    optimizer_type='adamw')\n","\n","#для предотвращения переобучения будем использовать callback, запоминающий веса модели на \n","#том шаге, которому соответствовала наилучшая точность прогноза на проверочной выборке\n","callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy', restore_best_weights=True, patience=5)\n","\n","preprocessor = create_preprocessor(preprocessor_name)\n","train_dataset = create_dataset(train_data, preprocessor, is_train=True)\n","valid_dataset = create_dataset(valid_data, preprocessor, is_train=False)\n","\n","with strategy.scope():\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    metric = tf.keras.metrics.SparseCategoricalAccuracy(\n","        'accuracy', dtype=tf.float32)\n","\n","\n","    model = create_encoder(encoder_name)\n","    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","    history = model.fit(\n","        x=train_dataset, validation_data=valid_dataset,\n","        steps_per_epoch=steps_per_epoch, epochs=epochs, callbacks=[callback],\n","        validation_steps=validation_steps)\n","\n","    print(model.evaluate(valid_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:585: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n","  [n for n in tensors.keys() if n not in ref_input_names])\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"AdamWeightDecay/gradients/PartitionedCall:1\", shape=(None,), dtype=int32), values=Tensor(\"clip_by_global_norm/clip_by_global_norm/_0:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"AdamWeightDecay/gradients/PartitionedCall:2\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"shape. This may consume a large amount of memory.\" % value)\n"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 77s 708ms/step - loss: 0.6463 - accuracy: 0.7100 - val_loss: 0.3116 - val_accuracy: 0.8932\n","Epoch 2/20\n","50/50 [==============================] - 27s 548ms/step - loss: 0.2119 - accuracy: 0.9444 - val_loss: 0.3436 - val_accuracy: 0.9036\n","Epoch 3/20\n","50/50 [==============================] - 27s 547ms/step - loss: 0.1567 - accuracy: 0.9613 - val_loss: 0.3155 - val_accuracy: 0.9245\n","Epoch 4/20\n","50/50 [==============================] - 27s 547ms/step - loss: 0.0644 - accuracy: 0.9862 - val_loss: 0.4275 - val_accuracy: 0.9036\n","Epoch 5/20\n","50/50 [==============================] - 27s 546ms/step - loss: 0.0283 - accuracy: 0.9950 - val_loss: 0.3982 - val_accuracy: 0.9297\n","Epoch 6/20\n","50/50 [==============================] - 27s 547ms/step - loss: 0.0258 - accuracy: 0.9962 - val_loss: 0.3907 - val_accuracy: 0.9401\n","Epoch 7/20\n","50/50 [==============================] - 27s 546ms/step - loss: 0.0198 - accuracy: 0.9969 - val_loss: 0.3912 - val_accuracy: 0.9271\n","Epoch 8/20\n","50/50 [==============================] - 27s 545ms/step - loss: 0.0129 - accuracy: 0.9981 - val_loss: 0.4017 - val_accuracy: 0.9349\n","Epoch 9/20\n","50/50 [==============================] - 27s 546ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.4257 - val_accuracy: 0.9297\n","Epoch 10/20\n","50/50 [==============================] - 27s 548ms/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.4303 - val_accuracy: 0.9401\n","Epoch 11/20\n","50/50 [==============================] - 27s 547ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.4417 - val_accuracy: 0.9375\n","13/13 [==============================] - 11s 795ms/step - loss: 0.4256 - accuracy: 0.9350\n","[0.42562803626060486, 0.9350000023841858]\n","CPU times: user 45.6 s, sys: 54.4 s, total: 1min 40s\n","Wall time: 7min 52s\n"]}]},{"cell_type":"markdown","metadata":{"id":"78UDUkzg-mlV"},"source":["*Примечание: ввиду стохастического характера процесса обучения нейросети результаты могут варьироваться от запуска к запуску. Инициализация всех псевдогенераторов случайных чисел (random.seed, numpy.random.seed, tensorflow.random.set_seed) данной проблемы не решает. На kagge были отправлены результаты прогноза модели, для которой val_accuracy после обучения составляла 0.9425*"]},{"cell_type":"markdown","metadata":{"id":"JRqmTqaLBDc6"},"source":["Конкатенируем два блока - препроцессор и энкодер - в единую модель для прогнозирования тестовых данных."]},{"cell_type":"code","metadata":{"id":"3jyRjCawqggV"},"source":["outputs = model(preprocessor(preprocessor.inputs))\n","trained_model = tf.keras.Model(preprocessor.inputs, outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFnZq0VosA2x"},"source":["Загружаем тестовую выборку и вычисляем прогноз."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRXm_h-fYEYy","executionInfo":{"status":"ok","timestamp":1630907826760,"user_tz":-180,"elapsed":2349156,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"6dd92f1a-14da-4989-80fd-20239f7e47be"},"source":["%%time\n","test_data = pd.read_csv(fname_test, sep='\\t')\n","test_dataset = tf.data.Dataset.from_tensor_slices(test_data['text'])\n","test_dataset = test_dataset.map(lambda x: [[x]])\n","predictions = []\n","for i, row in tqdm(enumerate(test_dataset)):\n","    # print(i, result, row[0].numpy()[0])\n","    predictions.append(tf.argmax(trained_model(row), axis=1)[0].numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function <lambda> at 0x7f9be2792710> and will run it as-is.\n","Cause: could not parse the source code of <function <lambda> at 0x7f9be2792710>: no matching AST found\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stderr","text":["500it [39:08,  4.70s/it]"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 19.8 s, sys: 3.61 s, total: 23.4 s\n","Wall time: 39min 8s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"aDOrHXiKsKgJ"},"source":["Сохраняем прогноз в файл для отправки на kaggle."]},{"cell_type":"code","metadata":{"id":"92hJO42SiWJ8"},"source":["submission = pd.DataFrame({'y': predictions})\n","submission.index.name = 'Id'\n","submission.to_csv('submission.csv')"],"execution_count":null,"outputs":[]}]}