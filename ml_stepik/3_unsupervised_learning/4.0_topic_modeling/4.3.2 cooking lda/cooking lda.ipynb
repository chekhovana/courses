{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"cooking lda.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ks6jxGY_tZp3"},"source":["# Programming Assignment: \n","## Готовим LDA по рецептам"]},{"cell_type":"markdown","metadata":{"id":"vkow3o88tZqA"},"source":["Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n","\n","Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n","\n","*pip install gensim*\n","\n","Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."]},{"cell_type":"markdown","metadata":{"id":"ptphNQ9_tZqB"},"source":["### Загрузка данных"]},{"cell_type":"markdown","metadata":{"id":"qVE_dktftZqC"},"source":["Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"]},{"cell_type":"code","metadata":{"id":"WTLI7KdptZqE"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InrffgTst4PD","executionInfo":{"status":"ok","timestamp":1614696381556,"user_tz":-180,"elapsed":193103,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"445c32ae-d824-4ff0-9345-96d36c43fc06"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Colab Notebooks/stepik/module3/4 topic modeling/cooking lda/'\n","%cd $path"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/stepik/module3/4 topic modeling/cooking lda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gr2WoqFetZqF"},"source":["with open(\"recipes.json\") as f:\n","    recipes = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeDAmBsQtZqG","executionInfo":{"status":"ok","timestamp":1614696382666,"user_tz":-180,"elapsed":194195,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"d46c05cc-d0ae-4198-dd71-8167a5019adf"},"source":["print(recipes[0])\n","print(len(recipes))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n","39774\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rLrUDa8ftZqG"},"source":["### Составление корпуса"]},{"cell_type":"code","metadata":{"id":"Sq0W815etZqH"},"source":["from gensim import corpora, models\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcciWSg6tZqI"},"source":["Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n","\n","[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n","\n","Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."]},{"cell_type":"code","metadata":{"id":"69X7EY0AtZqI"},"source":["texts = [recipe[\"ingredients\"] for recipe in recipes]\n","dictionary = corpora.Dictionary(texts)   # составляем словарь\n","corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yl5brNTbtZqJ","executionInfo":{"status":"ok","timestamp":1614696385506,"user_tz":-180,"elapsed":197010,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"52e4068d-e229-4747-e663-fea8c4500ef0"},"source":["print(texts[0])\n","print(corpus[0])\n","print(type(corpus))\n","corpus_size = 0\n","for d in corpus:\n","    for t in d:\n","        corpus_size += 1#t[1]\n","\n","print(corpus_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n","[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n","<class 'list'>\n","428249\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bdXf1rc6crw2"},"source":["cs = sum(len(d) for d in corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8i9LiCbtZqJ"},"source":["У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."]},{"cell_type":"markdown","metadata":{"id":"hFagjLZptZqJ"},"source":["### Обучение модели\n","Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."]},{"cell_type":"markdown","metadata":{"id":"7_Lhrv5DtZqK"},"source":["__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n","\n","\n","Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n","\n","Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n","\n","Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n","\n","У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzLJdXSBtZqK","executionInfo":{"status":"ok","timestamp":1614696449541,"user_tz":-180,"elapsed":261025,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"f44d853f-ade7-4bd4-9ed2-1dcca71cae1b"},"source":["from gensim.models import LdaModel\n","np.random.seed(76543)\n","# здесь код для построения модели:\n","%time ldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=40, passes=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 3s, sys: 66.3 ms, total: 1min 3s\n","Wall time: 1min 4s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxEr82NltZqL","executionInfo":{"status":"ok","timestamp":1614696516226,"user_tz":-180,"elapsed":632,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"a327d7f4-6c43-4574-d1ec-7e194db3c654"},"source":["words_to_explore = {k: 0 for k in [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]}\n","for t, top_words in ldamodel.show_topics(num_topics=40, num_words=10):\n","    words = ldamodel.get_topic_terms(t)\n","    for word_id, _ in words:\n","        word = dictionary.id2token[word_id]\n","        if word in words_to_explore:\n","            words_to_explore[word] += 1\n","print(words_to_explore)\n","print(list(words_to_explore.values()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'salt': 23, 'sugar': 9, 'water': 10, 'mushrooms': 0, 'chicken': 1, 'eggs': 2}\n","[23, 9, 10, 0, 1, 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aDq8MkhptZqL"},"source":["def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n","    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n","        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2F3kQlgMtZqM"},"source":["### Фильтрация словаря\n","В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."]},{"cell_type":"code","metadata":{"id":"P9C9i9hftZqM"},"source":["import copy\n","dictionary2 = copy.deepcopy(dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1Sos-J1tZqN"},"source":["__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n","\n","Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n","\n","Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFzInQTotZqO","executionInfo":{"status":"ok","timestamp":1614696529902,"user_tz":-180,"elapsed":830,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"1bb0c85f-e50f-4114-d70b-a22e95578d68"},"source":["dictionary2 = copy.deepcopy(dictionary)\n","dict_size_before = len(dictionary2.dfs)\n","frequently_used_words = [k for k, v in dictionary2.dfs.items() if v > 4000]\n","dictionary2.filter_tokens(frequently_used_words)\n","dict_size_after = len(dictionary2.dfs)\n","print(dict_size_before, dict_size_after)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6714 6702\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgi2xc7OtZqO","executionInfo":{"status":"ok","timestamp":1614696534393,"user_tz":-180,"elapsed":1288,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"508dc86a-d0be-48a4-ddb9-92c1c3634e42"},"source":["corpus_size_before = sum(len(d) for d in corpus)\n","corpus2 = [dictionary2.doc2bow(text) for text in texts]  # составляем корпус документов\n","corpus_size_after = sum(len(d) for d in corpus2)\n","print(corpus_size_before, corpus_size_after)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["428249 343665\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgVrsOW2tZqO","executionInfo":{"status":"ok","timestamp":1614696537267,"user_tz":-180,"elapsed":640,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"cfbf643a-5351-411f-fbde-61b99f7cf351"},"source":["print(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6714 6702 428249 343665\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lg-aYMjxtZqO"},"source":["def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n","    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n","        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGgCv3SMtZqP"},"source":["### Сравнение когерентностей\n","__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n","\n","Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_Tv3x2JguVq","executionInfo":{"status":"ok","timestamp":1614696599398,"user_tz":-180,"elapsed":11530,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"dcb4650d-ac55-4bc5-e65c-a801d6341f13"},"source":["np.random.seed(76543)\n","%time ldamodel2 = models.ldamodel.LdaModel(corpus2, id2word=dictionary, num_topics=40, passes=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 56.8 s, sys: 101 ms, total: 56.9 s\n","Wall time: 57.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTu7TpRmhT2D","executionInfo":{"status":"ok","timestamp":1614696600573,"user_tz":-180,"elapsed":1182,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"d6d3a644-2a11-45fb-a5ea-d5e1e5606798"},"source":["for m, c in zip([ldamodel, ldamodel2], [corpus, corpus2]):\n","    print(np.average([x[1] for x in m.top_topics(c)]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-5.849971009856026\n","-8.40790967936949\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164},"id":"JdA9OBYlhpRf","executionInfo":{"status":"error","timestamp":1614696601497,"user_tz":-180,"elapsed":929,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"d6f3e62c-c1d8-4f80-dceb-713ea5e8279a"},"source":["save_answers3(coherence, coherence2)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-6fb101e32223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_answers3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'save_answers3' is not defined"]}]},{"cell_type":"code","metadata":{"id":"3_V4jGRrtZqQ"},"source":["def save_answers3(coherence, coherence2):\n","    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n","        print(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6AbD2GBtZqQ"},"source":["Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "]},{"cell_type":"markdown","metadata":{"id":"3cPyCknQtZqQ"},"source":["### Изучение влияния гиперпараметра alpha"]},{"cell_type":"markdown","metadata":{"id":"1TfcVfWxtZqR"},"source":["В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n","\n","Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"]},{"cell_type":"code","metadata":{"id":"WdzYH14ttZqR"},"source":["ldamodel2.get_document_topics(corpus2[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jbGleUeJtZqS"},"source":["Также выведите содержимое переменной *.alpha* второй модели:"]},{"cell_type":"code","metadata":{"id":"rhyGziM1tZqS"},"source":["ldamodel2.alpha"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETrp-871tZqS"},"source":["У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."]},{"cell_type":"markdown","metadata":{"id":"0Czt38lTtZqT"},"source":["__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."]},{"cell_type":"code","metadata":{"id":"STaEpX96tZqT"},"source":["np.random.seed(76543)\n","%time ldamodel3 = models.ldamodel.LdaModel(corpus2, id2word=dictionary2, num_topics=40, passes=5, alpha=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HffqJvQTtZqT"},"source":["for m in ldamodel2, ldamodel3:\n","    cnt = 0\n","    for doc in corpus2:\n","        cnt+= len(m.get_document_topics(doc, minimum_probability=0.01))\n","    print(cnt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHuGopOotZqU"},"source":["def save_answers4(count_model2, count_model3):\n","    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n","        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47oh-Z3DtZqU"},"source":["Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."]},{"cell_type":"markdown","metadata":{"id":"zmOah5uktZqU"},"source":["### LDA как способ понижения размерности\n","Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n","\n","__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."]},{"cell_type":"code","metadata":{"id":"hM1kO_EFtZqV"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zI_ZqzgtZqV","executionInfo":{"status":"ok","timestamp":1614698011792,"user_tz":-180,"elapsed":41624,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"7bf4f9b9-c252-406b-dc31-1470e909cc0f"},"source":["X = np.zeros((len(corpus2), 40))\n","print(X.shape)\n","print(len(corpus2))\n","for i, doc in enumerate(corpus2):\n","    probs = ldamodel2.get_document_topics(doc)\n","    for p in probs:\n","        X[i, p[0]] = p[1]\n","y = [r['cuisine'] for r in recipes]\n","\n","rfc = RandomForestClassifier()\n","acc = cross_val_score(rfc, X, y, cv=3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(39774, 40)\n","39774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Fdy1-3hhI7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614698303638,"user_tz":-180,"elapsed":792,"user":{"displayName":"Надежда Чехова","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSJHZgegxNfZHz-DHNitm4K7fL7sLHVtvAWS50Ceg=s64","userId":"05232737027527319731"}},"outputId":"bebd1790-6646-40d0-90a0-bbeba822d150"},"source":["np.average(acc)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5558404988183235"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"8PByZm7ftZqX"},"source":["def save_answers5(accuracy):\n","     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n","        fout.write(str(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjEAlQAttZqX"},"source":["Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."]},{"cell_type":"markdown","metadata":{"id":"jJTOht_RtZqY"},"source":["### LDA — вероятностная модель\n","Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n","\n","Для документа $d$ длины $n_d$:\n","1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n","1. Для каждого слова $w = 1, \\dots, n_d$:\n","    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n","    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n","    \n","Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n","\n","В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"]},{"cell_type":"code","metadata":{"id":"CAe0jFFbtZqY"},"source":["def generate_recipe(model, num_ingredients):\n","    theta = np.random.dirichlet(model.alpha)\n","    for i in range(num_ingredients):\n","        t = np.random.choice(np.arange(model.num_topics), p=theta)\n","        topic = model.show_topic(t, topn=model.num_terms)\n","        topic_distr = [x[1] for x in topic]\n","        terms = [x[0] for x in topic]\n","        w = np.random.choice(terms, p=topic_distr)\n","        print(w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVAgHPVqtZqY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbSA10L6tZqZ"},"source":["### Интерпретация построенной модели\n","Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n","\n","Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."]},{"cell_type":"code","metadata":{"id":"3hphzh7utZqZ"},"source":["import pandas\n","import seaborn\n","from matplotlib import pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3jCbE6ZtZqa"},"source":["def compute_topic_cuisine_matrix(model, corpus, recipes):\n","    # составляем вектор целевых признаков\n","    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n","    # составляем матрицу\n","    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n","    for recipe, bow in zip(recipes, corpus):\n","        recipe_topic = model.get_document_topics(bow)\n","        for t, prob in recipe_topic:\n","            tc_matrix[recipe[\"cuisine\"]][t] += prob\n","    # нормируем матрицу\n","    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n","    for recipe in recipes:\n","        target_sums[recipe[\"cuisine\"]] += 1\n","    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gtjbsp51tZqa"},"source":["def plot_matrix(tc_matrix):\n","    plt.figure(figsize=(10, 10))\n","    seaborn.heatmap(tc_matrix, square=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IlkVQunWtZqb"},"source":["# Визуализируйте матрицу\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIeIZnumtZqb"},"source":["Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "]},{"cell_type":"markdown","metadata":{"id":"CFCgTktatZqb"},"source":["Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."]},{"cell_type":"markdown","metadata":{"id":"Y_ys8JdMtZqb"},"source":["### Заключение\n","В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "]}]}